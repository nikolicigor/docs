---
title: "SFTP Flow"
description: "Complete guide to integrating with FileFeed through SFTP, webhooks, and REST API"
---

# SFTP Integration

Complete guide to integrating with FileFeed through SFTP, webhooks, and REST API

## SFTP Integration

FileFeed provides a comprehensive SFTP-based integration platform that automates 
file-based workflows by receiving, validating, transforming, and routing structured 
data files. This section covers all aspects of SFTP integration including core 
concepts, webhook listeners, REST API access, and implementation checklists.

## Core Concepts

Understanding the fundamental concepts of FileFeed platform is essential for 
successful integration. This section explains how clients, schemas, and pipelines 
work together to automate your file processing workflows.

### 1. Platform Overview

FileFeed is designed to solve the challenge of managing file transfers and data 
integrations across multiple clients, systems, and file formats. It provides a 
centralized platform where you can:

- Create dedicated SFTP spaces for each client or data source
- Define schemas that validate incoming data files
- Build automated pipelines that transform files into standardized formats
- Send processed data to destination systems via webhooks or API
- Monitor file processing and alert on errors

### 2. Clients

In FileFeed, a **Client** represents an 
organization or business entity that interacts with your system through file 
transfers. Each client has:

- **Dedicated SFTP space**: A secure 
  environment where the client can upload files
- **SFTP credentials**: Username and 
  password for connecting to the SFTP server
- **SFTP host**: The server address 
  displayed on the client's page for connection
- **Client ID**: A unique identifier 
  used in API calls and data routing
- **Associated pipelines**: Workflows 
  that process files uploaded by this client

Clients are isolated from each other, ensuring data privacy and security. 
Clients can connect to their dedicated SFTP space using their assigned username 
and password with the host specified on their configuration page.

```json
// Example client object
{
  "clientId": "c4b3f495-5dfc-4a91-b604-a8e66ab4a220",
  "clientName": "acme_logistics",
  "displayName": "Acme Logistics Inc.",
  "status": "active",
  "created": "2023-05-15T10:30:00Z",
  "lastActive": "2023-06-01T08:45:22Z",
  "sftpUsername": "acme_sftp"
}
```

### 3. Schemas

A **Schema** defines the structure and 
validation rules for data files. It specifies:

- **Fields**: The columns or properties 
  expected in the data
- **Data types**: The expected type for 
  each field (string, number, date, etc.)
- **Validation rules**: Requirements for 
  each field (required/optional, format, range, etc.)

Schemas help ensure data quality by rejecting files that don't meet your 
specifications. They define what data is expected and its format, but do not 
handle transformations (which are handled separately by pipelines).

```json
// Example schema definition
{
  "schemaId": "ord-schema-v1",
  "name": "Order Schema v1",
  "fileType": "csv",
  "fields": [
    {
      "name": "order_id",
      "type": "string",
      "required": true,
      "validation": {
        "pattern": "^ORD-[0-9]{6}$"
      }
    },
    {
      "name": "customer_email",
      "type": "string",
      "required": true,
      "validation": {
        "format": "email"
      }
    },
    {
      "name": "order_date",
      "type": "date",
      "required": true,
      "sourceFormat": "MM/DD/YYYY",
      "targetFormat": "YYYY-MM-DD"
    },
    {
      "name": "total_amount",
      "type": "number",
      "required": true,
      "validation": {
        "min": 0
      }
    }
  ]
}
```

### 4. Pipelines

A **Pipeline** in FileFeed defines how 
files are processed when uploaded to a specific folder. Each pipeline includes:

- **Schema**: The file structure we're 
  mapping to (defines expected data format)
- **Webhook**: Optional notification when 
  new files are uploaded
- **Starter file**: Template that defines 
  the expected header structure for Excel/CSV files
- **Mappings**: Column mappings (both 
  automatic and manual) from source to target schema
- **Transformations**: Data manipulations 
  that can be applied to specific columns

Each pipeline creates a dedicated folder in the client's SFTP space. When a file 
is uploaded to this folder, it automatically triggers the processing based on the 
defined mappings and transformations.

```json
// Example pipeline configuration
{
  "options": {
    "delimiter": ",",
    "skipHeaderRow": true
  },
  "fieldMappings": [
    {
      "source": "customer_id",
      "target": "id"
    },
    {
      "source": "customer_name",
      "target": "name"
    },
    {
      "source": "customer_email",
      "target": "email",
      "transform": "toLowerCase"
    },
    {
      "source": "customer_phone",
      "target": "phone",
      "transform": "formatPhoneNumber"
    }
  ],
  "transformations": {
    "toLowerCase": "function(value) { return value.toLowerCase(); }",
    "formatPhoneNumber": "function(value) { return value.replace(/[^0-9]/g, ''); }"
  }
}
```

### 5. Typical Workflow

Here's a typical workflow in FileFeed that illustrates how clients, schemas, 
and pipelines work together:

1. **Client Setup**: You create a new 
   client in FileFeed, which generates SFTP credentials and a dedicated SFTP space.
2. **Schema Definition**: You define a 
   schema that specifies the target data structure you want to receive after 
   processing.
3. **Pipeline Creation**: You create a 
   pipeline and associate it with the schema. This automatically creates a 
   dedicated folder in the client's SFTP space.
4. **Starter File Setup**: You upload a 
   template file with the expected header structure for the files your client 
   will upload.
5. **Field Mapping**: You define mappings 
   between source columns in the uploaded files and target fields in your schema.
6. **Transformation Setup**: You create 
   JavaScript transformation functions to apply to specific fields.
7. **Webhook Configuration**: You set up 
   an optional webhook URL to receive notifications when files are processed.
8. **File Upload**: The client uploads a 
   file to their dedicated pipeline folder in the SFTP space.
9. **Automatic Processing**: FileFeed 
   detects the new file, applies the defined mappings and transformations.
10. **Notification**: If a webhook is 
    configured, a notification is sent to the specified URL.

## Webhook Listener

When files are processed in FileFeed, our system can notify your application 
through webhooks. Follow these steps to implement a webhook listener:

### 1. Understanding Webhooks

FileFeed webhooks send HTTP POST requests to your specified endpoint when 
certain events occur:

- **GENERAL:** Sent for all file processing events, including 
  successful processing and error situations

### 2. Webhook Request Structure

Below is an example of the JSON payload sent to your webhook endpoint.

```json
{
  "event": "GENERAL",
  "timestamp": "2025-05-19T21:30:00.000Z",
  "data": {
    "fileId": "550e8400-e29b-41d4-a716-446655440000",
    "filename": "example_data.csv",
    "clientName": "acme_inc",
    "status": "completed",
    "processedFilename": "example_data_processed.csv",
    "jsonFilename": "example_data.json",
    "size": 24680,
    "processedAt": "2025-05-19T21:29:55.000Z"
  }
}
```

### 3. Webhook Security

All webhook requests include a signature to verify authenticity:

- Requests contain an `x-sftpsync-signature` header
- The signature is an HMAC-SHA256 hash of the request body using your webhook secret
- The secret can be found in the webhook configuration section of your dashboard

### 4. Example (Node.js)

Here's a basic Node.js example using Express to listen for FileFeed webhooks 
and verify their signatures.

```javascript
const express = require('express');
const crypto = require('crypto');
const app = express();

app.use(express.json());

const WEBHOOK_SECRET = 'your_webhook_secret';

function verifySignature(requestPayload, signature) {
  const hmac = crypto.createHmac('sha256', WEBHOOK_SECRET);
  const digest = hmac.update(JSON.stringify(requestPayload)).digest('hex');
  return crypto.timingSafeEqual(
    Buffer.from(digest),
    Buffer.from(signature)
  );
}

app.post('/webhooks/sftpsync', (req, res) => {
const signature = req.headers['x-sftpsync-signature'];
  const requestBody = req.body;

  if (!signature || !verifySignature(requestBody, signature)) {
    return res.status(401).send('Invalid signature');
  }

  const event = requestBody.event;

  switch (event) {
    case 'GENERAL': {
      const { filename, status } = requestBody.data;
      console.log(`File ${filename} event received with status: ${status}`);
      
      if (status === 'completed') {
        console.log(`File ${filename} processed successfully`);
      } else if (status === 'failed') {
        const errorMessage = requestBody.data.errorMessage || 'Unknown error';
        console.log(`File ${filename} processing failed: ${errorMessage}`);
      }
      break;
    }
    default:
      console.log(`Unknown event type: ${event}`);
  }

  res.status(200).send('Webhook received');
});

app.listen(3000, () => {
  console.log('Webhook listener running on port 3000');
});
```

### 5. Best Practices

- Respond quickly (within 5 seconds) to avoid webhook timeouts
- Implement idempotency to handle potential duplicate webhook deliveries
- Use a queue system for processing webhook data asynchronously
- Store your webhook secret securely
- Implement proper error handling

## REST API

FileFeed provides a REST API endpoint to fetch processed JSON data from your files.

### 1. Retrieving Files by Pipeline Run ID

To retrieve the data for a specific pipeline run, use the 
`/files/pipeline-runs/:id` endpoint. This is useful when you need to 
fetch data based on a specific processing job rather than searching by client 
and file names.

You can find pipeline run IDs in two ways:

- **In the app:** Navigate to the Pipeline 
  Runs tab in your dashboard to view and copy pipeline run IDs
- **Via webhook:** Pipeline run IDs are 
  included in webhook event payloads

**Request Example:**

```bash
curl -X GET "https://api.filefeed.io/files/pipeline-runs/run_12345?offset=0&limit=10" \
  -H "X-API-Key: your_api_key_here"
```

**Response Example:**

```json
{
  "data": [
    { "id": "123", "name": "Product Alpha", "email": "contact@example.com" },
    { "id": "124", "name": "Product Beta", "email": "support@example.com" }
  ],
  "pagination": { "offset": 0, "limit": 10, "total": 2 }
}
```

<Note>
All API requests require authentication using your API key, which you can find 
in the dashboard under **My Account** → **Security Settings** → **API Key** (click "Reveal").
</Note>

## Integration Checklist

Use this checklist to ensure your FileFeed integration is properly configured 
and ready for production use.

### Setup

- [ ] Obtain your API key from "My Account → Security Settings → API Key"
- [ ] Create a webhook endpoint and register it in the FileFeed dashboard
- [ ] Get your webhook secret from the webhook configuration page
- [ ] Implement signature verification for webhook security

### Configuration

- [ ] Create client accounts for each data source
- [ ] Generate SFTP credentials for each client
- [ ] Define schemas for expected data formats
- [ ] Create pipelines with proper field mappings
- [ ] Upload starter files with correct headers
- [ ] Set up webhook URLs for notifications

### Production

- [ ] Test file uploads and processing
- [ ] Store API keys securely
- [ ] Set up monitoring for webhook delivery failures
- [ ] Test error handling scenarios
- [ ] Validate data accuracy with sample files

<Tip>
Once you've completed all items in this checklist, your FileFeed integration 
should be ready for production use.
</Tip>
